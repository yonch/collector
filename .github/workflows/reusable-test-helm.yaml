name: reusable-test-helm

on:
  workflow_call:
    inputs:
      instance-type:
        description: EC2 instance type
        required: false
        default: m7i.xlarge
        type: string
      collector-image-artifact:
        description: Name of collector image artifact to use
        required: false
        default: collector-image
        type: string
      nri-init-image-artifact:
        description: Name of nri-init image artifact to use
        required: false
        default: nri-init-image
        type: string
      collector-image-repo:
        description: Collector image repository (e.g., local/collector)
        required: false
        default: local/collector
        type: string
      collector-image-tag:
        description: Collector image tag
        required: true
        type: string
      nri-init-image-repo:
        description: NRI init image repository (e.g., local/nri-init)
        required: false
        default: local/nri-init
        type: string
      nri-init-tag:
        description: NRI init image tag
        required: true
        type: string
      s3-bucket:
        description: S3 bucket to validate outputs
        required: false
        default: unvariance-collector-test-irsa
        type: string
      region:
        description: AWS region
        required: false
        type: string
    secrets:
      AWS_ROLE_ARN:
        required: false
      REPO_ADMIN_TOKEN:
        required: false
      AWS_REGION:
        required: false

jobs:
  setup-runner:
    name: Start EC2 runner
    runs-on: ubuntu-latest
    outputs:
      runner-label: ${{ steps.start-runner.outputs.runner-label }}
      ec2-instance-id: ${{ steps.start-runner.outputs.ec2-instance-id }}
      region: ${{ steps.start-runner.outputs.region }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      - name: Start AWS Runner
        id: start-runner
        uses: ./.github/actions/aws-runner
        with:
          github-token: ${{ secrets.REPO_ADMIN_TOKEN }}
          aws-role-arn: ${{ secrets.AWS_ROLE_ARN }}
          iam-role-name: github-actions-runner
          instance-type: ${{ inputs.instance-type }}
          image-type: ubuntu-24.04

  k3s-deployment:
    needs: [setup-runner]
    runs-on: ${{ needs.setup-runner.outputs.runner-label }}
    timeout-minutes: 10
    env:
      HOME: /root
    steps:
      - name: Checkout .github folder
        uses: actions/checkout@v4
        with:
          fetch-depth: 1
          sparse-checkout: |
            .github
          sparse-checkout-cone: true
      - name: Create HOME directory
        run: |
          mkdir -p $HOME

      - name: Setup k3s cluster
        uses: ./.github/actions/setup-k3s
        with:
          kubeconfig_path: /etc/rancher/k3s/k3s.yaml
          wait_kube_system: true
          timeout_api_server_ready_seconds: 300
          timeout_node_ready_seconds: 300
          timeout_kube_system_each_seconds: 10
          max_retries_kube_system_ready: 10

      - name: Get Default objects in kube-system
        run: | 
          kubectl get all -n kube-system

      - name: Install Helm
        run: |
          curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

      - name: Install awscli
        uses: ./.github/actions/setup-awscli

  helm-chart-deployment:
    needs: [setup-runner, k3s-deployment]
    runs-on: ${{ needs.setup-runner.outputs.runner-label }}
    timeout-minutes: 15
    strategy:
      matrix:
        trace-mode: [false, true]
    env:
      RELEASE_NAME: collector-${{ matrix.trace-mode == true && 'trace' || 'aggregated' }}
      S3_BUCKET: "unvariance-collector-test-irsa"  # Same bucket used in IAM role testing
      AWS_REGION: ${{ secrets.AWS_REGION }}
      KUBECONFIG: /etc/rancher/k3s/k3s.yaml
      HOME: /root
      # Use provided image repos/tags (defaults are local/*)
      IMAGE_REPOSITORY: ${{ inputs.collector-image-repo }}
      IMAGE_TAG: ${{ inputs.collector-image-tag || 'latest' }}
      NRI_INIT_REPOSITORY: ${{ inputs.nri-init-image-repo }}
      NRI_INIT_TAG: ${{ inputs.nri-init-tag || 'latest' }}
      TRACE_MODE: ${{ matrix.trace-mode }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download collector image artifact
        uses: actions/download-artifact@v4
        with:
          name: ${{ inputs.collector-image-artifact }}
          path: collector-image

      - name: Download nri-init image artifact
        uses: actions/download-artifact@v4
        with:
          name: ${{ inputs.nri-init-image-artifact }}
          path: nri-init-image

      - name: Import images into k3s containerd
        run: |
          set -euxo pipefail
          # Import docker image tars into k3s' containerd
          k3s ctr images import collector-image/image.tar
          k3s ctr images import nri-init-image/image.tar
          # Show images
          k3s ctr -n k8s.io images ls | sed -n '1,200p'

      - name: Generate UUID Prefix
        id: generate-uuid
        run: |
          UUID=$(python3 -c "import uuid; print(uuid.uuid4())")
          echo "Using UUID prefix: $UUID"
          echo "uuid=$UUID" >> $GITHUB_OUTPUT

      - name: Deploy Collector Helm Chart
        run: |
          UUID_PREFIX="${{ steps.generate-uuid.outputs.uuid }}-"
          
          # Create values override file
          cat > values-override.yaml << EOF
          image:
            repository: "${IMAGE_REPOSITORY}"
            tag: "${IMAGE_TAG}"
            
          collector:
            verbose: true
            trace: ${TRACE_MODE}
          
          storage:
            type: "s3"
            prefix: "${UUID_PREFIX}"
            s3:
              bucket: "${S3_BUCKET}"
              region: "${AWS_REGION}"
              auth:
                method: "iam"  # Using IAM role
          
          nri:
            init:
              image:
                repository: "${NRI_INIT_REPOSITORY}"
                tag: "${NRI_INIT_TAG}"
          EOF
          
          # Print the values being used
          echo "Using image: ${IMAGE_REPOSITORY}:${IMAGE_TAG}"
          
          # Install the helm chart
          helm upgrade --install ${RELEASE_NAME} ./charts/collector -f values-override.yaml

      - name: Wait for Collector Pods to be Ready
        run: |
          # Allow the wait to fail so we can dump diagnostics
          set +e
          kubectl wait --for=condition=Ready pods --timeout=60s -l app.kubernetes.io/name=collector
          STATUS=$?
          set -e
          if [ "$STATUS" -ne 0 ]; then
            echo "Collector pods are not ready after timeout. Dumping diagnostics..."
            echo "\n==> kubectl get pods (all)"
            kubectl get pods -o wide || true
            echo "\n==> kubectl describe pods (collector-labeled)"
            kubectl describe pods -l app.kubernetes.io/name=collector || true
            echo "\n==> kubectl get events (sorted)"
            kubectl get events --sort-by=.lastTimestamp || true
            echo "\n==> kubectl logs for collector pods (all containers)"
            for p in $(kubectl get pods -l app.kubernetes.io/name=collector -o jsonpath='{.items[*].metadata.name}'); do
              echo "\n---- Logs for pod: $p ----"
              kubectl logs "$p" --all-containers=true --tail=-1 || true
            done
            exit 1
          fi

      - name: Show Pod Status
        run: |
          kubectl get pods
          kubectl describe pods -l app.kubernetes.io/name=collector

      - name: Display logs while collector runs for a while
        run: |
          timeout 10s kubectl logs -f -l app.kubernetes.io/name=collector || true
      
      - name: Uninstall Collector Helm Chart
        run: |
          helm uninstall ${RELEASE_NAME} --wait --timeout=60s

      - name: Collector logs
        run: |
          kubectl logs -l app.kubernetes.io/name=collector || true
          
      - name: Check for Files in S3
        run: |
          UUID_PREFIX="${{ steps.generate-uuid.outputs.uuid }}"
          echo "Checking for files with prefix ${UUID_PREFIX} in S3 bucket ${S3_BUCKET}"
          
          # List files with the UUID prefix
          S3_FILES=$(aws s3 ls "s3://${S3_BUCKET}/${UUID_PREFIX}" --recursive || echo "")
          
          if [ -z "$S3_FILES" ]; then
            echo "No files found with prefix ${UUID_PREFIX} in bucket ${S3_BUCKET}"
            exit 1
          else
            echo "Found files with prefix ${UUID_PREFIX}:"
            echo "$S3_FILES"
            
            # Get the first file path
            FIRST_FILE=$(echo "$S3_FILES" | head -n 1 | awk '{print $4}')
            
            # Download the file for validation
            aws s3 cp "s3://${S3_BUCKET}/${FIRST_FILE}" /tmp/test-parquet.parquet
            
            # Check file size
            FILE_SIZE=$(stat -c %s /tmp/test-parquet.parquet)
            echo "Downloaded file size: ${FILE_SIZE} bytes"
            
            # We could add parquet validation here if a parquet tool is available
            echo "Helm chart S3 integration test successful"
          fi
      
      - name: Upload Test Results
        uses: actions/upload-artifact@v4
        with:
          name: helm-chart-test-results-${{ matrix.trace-mode == true && 'trace' || 'aggregated' }}
          path: /tmp/test-parquet.parquet
          if-no-files-found: warn

  verify-artifacts:
    name: Verify Parquet Artifacts
    needs: [helm-chart-deployment]
    runs-on: ubuntu-latest
    if: always()  # Run even if helm-chart-deployment fails
    strategy:
      matrix:
        trace-mode: [false, true]
    steps:
      - name: Download Artifacts
        uses: actions/download-artifact@v4
        with:
          name: helm-chart-test-results-${{ matrix.trace-mode == true && 'trace' || 'aggregated' }}
          path: parquet-data

      - name: Install pqrs
        run: |
          curl -L -o pqrs.zip https://github.com/manojkarthick/pqrs/releases/download/v0.3.2/pqrs-0.3.2-x86_64-unknown-linux-gnu.zip
          python3 -m zipfile -e pqrs.zip .
          sudo mv pqrs-0.3.2-x86_64-unknown-linux-gnu/bin/pqrs /usr/local/bin/
          sudo chmod +x /usr/local/bin/pqrs
          rm -rf pqrs.zip pqrs-0.3.2-x86_64-unknown-linux-gnu
          pqrs --version

      - name: Verify Parquet File Schema and Contents
        run: |
          echo "Verifying Parquet files for ${{ matrix.trace-mode == true && 'trace' || 'aggregated' }} mode..."
          
          # Find the parquet file
          PARQUET_FILE=$(find parquet-data -name "*.parquet" -type f | head -n 1)
          
          if [ -z "$PARQUET_FILE" ]; then
            echo "ERROR: No parquet file found in artifacts"
            exit 1
          fi
          
          echo "Found parquet file: $PARQUET_FILE"
          
          # Check file size
          FILE_SIZE=$(stat -c %s "$PARQUET_FILE")
          echo "File size: ${FILE_SIZE} bytes"
          
          if [ "$FILE_SIZE" -eq 0 ]; then
            echo "ERROR: Parquet file is empty"
            exit 1
          fi
          
          # Generate and examine schema
          echo "Generating schema..."
          pqrs schema "$PARQUET_FILE" > schema.txt
          cat schema.txt
          
          # Define field lists for different modes
          if [ "${{ matrix.trace-mode }}" = "true" ]; then
            echo "Setting up trace mode field verification..."
            REQUIRED_FIELDS=("pid" "timestamp" "cpu_id" "is_context_switch" "cgroup_id" "cache_references" "cycles" "instructions" "llc_misses" "next_tgid")
          else
            echo "Setting up aggregated mode field verification..."
            REQUIRED_FIELDS=("pid" "start_time" "cgroup_id" "cache_references" "cycles" "instructions" "llc_misses")
          fi

          # NRI enrichment fields are expected in both modes (nullable)
          ENRICH_FIELDS=("pod_name" "pod_namespace" "pod_uid" "container_name" "container_id")
          REQUIRED_FIELDS+=("${ENRICH_FIELDS[@]}")
          
          # Verify all required fields are present in schema (including NRI enrichment)
          echo "Verifying required fields in schema..."
          for field in "${REQUIRED_FIELDS[@]}"; do
            if ! grep -q "$field" schema.txt; then
              echo "ERROR: Required field '$field' not found in schema"
              exit 1
            else
              echo "✓ Found required field: $field"
            fi
          done
          
          # Sample records and verify content using JSON output
          echo "Sampling records with JSON output..."
          pqrs sample --records 100 --json "$PARQUET_FILE" > sample.json
          
          # Verify field values - check that each field has at least 2 different values
          echo "Verifying field values have sufficient diversity..."
          
          # Define fields that should only warn (VM might not support these performance counters)
          WARN_ONLY_FIELDS=("cache_references" "llc_misses")
          
          for field in "${REQUIRED_FIELDS[@]}"; do
            echo "Checking field: $field"
            
            # Extract all values for this field and count unique values
            UNIQUE_VALUES=$(jq -r ".${field} // null" sample.json | sort -u | wc -l)
            echo "  Unique values in $field: $UNIQUE_VALUES"
            
            if [ "$UNIQUE_VALUES" -lt 2 ]; then
              # Check if this field should only warn (performance counters that might not be supported on VM)
              if [[ " ${WARN_ONLY_FIELDS[*]} " =~ " ${field} " ]]; then
                echo "  WARNING: Field '$field' has less than 2 different values ($UNIQUE_VALUES) - this might indicate VM doesn't support this performance counter"
              else
                echo "  ERROR: Field '$field' has less than 2 different values ($UNIQUE_VALUES)"
                exit 1
              fi
            else
              echo "  ✓ Field '$field' has sufficient diversity"
            fi
          done
          
          echo "Parquet file verification completed successfully for ${{ matrix.trace-mode == true && 'trace' || 'aggregated' }} mode"


  stop-runner:
    name: Stop EC2 runner
    needs: [setup-runner, k3s-deployment, helm-chart-deployment]
    runs-on: ubuntu-latest
    if: always()  # Run even if previous jobs fail
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      - name: Stop AWS Runner
        uses: ./.github/actions/aws-runner/cleanup
        with:
          runner-label: ${{ needs.setup-runner.outputs.runner-label }}
          ec2-instance-id: ${{ needs.setup-runner.outputs.ec2-instance-id }}
          github-token: ${{ secrets.REPO_ADMIN_TOKEN }}
          aws-role-arn: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ needs.setup-runner.outputs.region }}
